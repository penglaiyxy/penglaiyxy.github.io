---
layout:     post
title:      OSD利用率高，集群却一切正常
subtitle:   周一晚上，大雨漂泊~~ 
date:       2018-05-07
author:     鱼鱼
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - CEPH
    - 技术
---

# 现象

监控系统发现有三个OSD连续一分钟内利用率超过80%

# 检查集群状态

ceph -s一切正常，osd没有下线上线， monitor工作正常

# 检查PG分布，各个主机中PG分布

PG分布也正常，主机中PG数量差不多，关键是OSD告警的三台主机其PG总数在平均水平之下

查看PG分布是用了如下链接的脚本：

http://www.zphj1987.com/2015/10/04/%E6%9F%A5%E8%AF%A2osd%E4%B8%8A%E7%9A%84pg%E6%95%B0/

# Filestore 数据合并或者拆分

在告警的OSD底层查看PG数据，没有明显的文件夹创建或者合并的痕迹，即使有，也没有日志，我觉得这是个缺陷，因为合并和拆分过程中有大量元数据操作，消耗硬盘利用率！

在查看PG数据时，发现最底层的文件夹DIR_X,其文件夹的Modify属性值都是在告警那段时间内，最终判断为在告警那段时间内，有大量的文件创建或者删除。

云平台上很难知道用户的真实操作，所以我觉得在filestore层加上合并拆分的日志很有必要。





